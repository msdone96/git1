name: Web Scraper Workflow

on:
  workflow_dispatch:      # Only manual triggering
    inputs:
      timeout:
        description: 'Request timeout in seconds'
        required: false
        default: '30'
        type: string
      workers:
        description: 'Number of concurrent workers'
        required: false
        default: '10'
        type: string
      batch_size:
        description: 'Batch size for processing'
        required: false
        default: '1000'
        type: string

env:
  GO_VERSION: '1.21.x'

jobs:
  scrape:
    name: Run Scraper
    runs-on: ubuntu-latest
    timeout-minutes: 350  # GitHub Actions maximum timeout limit

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: ${{ env.GO_VERSION }}
          cache: true

      - name: Initialize Go module
        run: |
          go mod tidy
          go get github.com/PuerkitoBio/goquery@v1.8.1
          go get golang.org/x/net/html/charset@v0.19.0
          go get golang.org/x/text/encoding/htmlindex@v0.14.0
          go mod tidy
          cat go.mod  # Debug: show go.mod contents
          cat go.sum  # Debug: show go.sum contents

      - name: Verify dependencies
        run: |
          go mod verify
          go mod download

      - name: Build scraper
        run: go build -v -o scraper

      - name: Verify input file
        run: |
          if [ ! -f "input.csv" ]; then
            echo "Creating sample input file..."
            echo "URL,Notes" > input.csv
            echo "https://example.com,Test site" >> input.csv
          fi
          echo "Input file contents:"
          cat input.csv

      - name: Run scraper
        run: |
          ./scraper \
            -timeout=${{ inputs.timeout }} \
            -workers=${{ inputs.workers }} \
            -batch=${{ inputs.batch_size }} \
            -input="input.csv" \
            -output-links="output_links.csv" \
            -output-summary="output_summary.csv"

      - name: Check outputs
        run: |
          echo "=== Checking outputs ==="
          if [ -f "output_links.csv" ]; then
            echo "Links file exists. First few lines:"
            head -n 5 output_links.csv
          else
            echo "Warning: output_links.csv not found"
          fi
          
          if [ -f "output_summary.csv" ]; then
            echo "Summary file exists. First few lines:"
            head -n 5 output_summary.csv
          else
            echo "Warning: output_summary.csv not found"
          fi

      - name: Upload results
        uses: actions/upload-artifact@v3
        if: always()  # Upload even if previous steps failed
        with:
          name: scraping-results-${{ github.run_number }}
          path: |
            output_links.csv
            output_summary.csv
          retention-days: 30
